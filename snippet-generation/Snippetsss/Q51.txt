/Users/rohitchawla/PycharmProjects/tryingtodosomecrap/Corpus/CACM-3075
fast parallel sorting algorithms a parallel bucket sort algorithm is presented that requires time olog n and the use of n processors the algorithm makes use of a technique that requires more space than the product of processors and time a realistic model is used model is used in which no memory contention is permitted a procedure is also presented to sort n numbers in time ok log n using n 1 1k processors for k an arbitrary integer the model of computation for this procedure permits simultaneous fetches from the same memory location cacm august 1978 hirschberg d parallel processing sorting algorithms bucket sort 374 434 525 531 ca780803 dh february 7 1979 1025 am 
****************************
/Users/rohitchawla/PycharmProjects/tryingtodosomecrap/Corpus/CACM-3156
computing connected components on parallel computers we present a parallel algorithm which uses n2 processors to find the connected components of an undirected graph with n vertices in time olog2n an olog2n time bound also can be achieved using only nnlog2n processors the algorithm can be used to find the transitive closure of a symmetric boolean matrix we assume that the processors have access to a common memory simultaneous access to the same location is permitted for fetch instructions but not for store instructions cacm august 1979 hirschberg d chandra a sarwate d graph theory parallel processing algorithms transitive closure connected component 525 532 622 ca790802 db january 4 1980 1218 pm 
****************************
/Users/rohitchawla/PycharmProjects/tryingtodosomecrap/Corpus/CACM-1811
a case study in programming for parallel processors an affirmative partial answer is provided to the question of whether it is possible to program parallel processor computing systems to efficiently decrease execution time for useful problems parallel processor systems are multiprocessor systems in which several of the processors can simultaneously execute separate tasks of a single job thus cooperating to decrease the solution time of a computational problem the processors have independent instruction counters meaning that each processor executes its own task program relatively independently of the other processors communication between cooperating processors is by means of data in storage shared by all processors a program for the determination of the distribution of current in an electrical network was written for a parallel processor computing system and execution of this program was simulated the data gathered from simulation runs demonstrate the efficient solution of this problem typical of a large class of important problems it is shown that with proper programming solution time when n processors are applied approaches 1n times the solution time for a single processor while improper programming can actually lead to an increase of solution time with the number of processors stability of the method of solution was also investigated cacm december 1969 rosenfeld j l parallel processor parallelism parallel programming multiprocessor multiprogramming tasking storage interference electrical network simulation relaxation jacobi gauss seidel convergence 324 49 514 517 621 ca691201 jb february 15 1978 445 pm 
****************************
/Users/rohitchawla/PycharmProjects/tryingtodosomecrap/Corpus/CACM-2714
merging with parallel processors consider two linearly ordered sets a b am bn mn and p pm parallel processors working synchronously the paper presents an algorithm for merging a and b with the p parallel processors which requires at most 2log2 2m13mp mplog2 nm steps if n 2bm b an integer the algorithm requires at most 2log2 m1 mp2b steps in the case where m and n are of the same order of magnitude ie nkm with k being a constant the algorithm requires 2log2 m1 mp3k steps these performances compare very favorably with the previous best parallel merging algorithm batchers algorithm which requires np mn2plog2 m steps in the general case and kmp k12mplog2 m in the special case where nkm cacm october 1975 gavril f parallel processing parallel merging parallel binary insertion 531 ca751005 jb january 6 1978 1050 am 
****************************
/Users/rohitchawla/PycharmProjects/tryingtodosomecrap/Corpus/CACM-3006
anomalies with variable partition paging algorithms five types of anomalous behavior which may occur in paged virtual memory operating systems a redefined one type of anomaly for example concerns the fact that with certain reference strings and paging algorithms an increase in mean memory allocation may result in an increase in fault rate two paging algorithms are examined in terms of their anomaly potential and reference string examples of various anomalies are presented two paging algorithm properties the inclusion property and the generalized inclusion property are discussed and the anomaly implications of these properties presented cacm march 1978 franklin m graham g gupta r anomaly memory management program behavior stack algorithms virtual memory working set page fault frequency paging algorithms 432 435 46 81 ca780307 jb march 28 1978 101 pm 
****************************
/Users/rohitchawla/PycharmProjects/tryingtodosomecrap/Corpus/CACM-2723
multiprocessing compactifying garbage collection algorithms for a multiprocessing compactifying garbage collector are presented and discussed the simple case of two processors one performing lisp like list operations and the other performing garbage collection continuously is thoroughly examined the necessary capabilities of each processor are defined as well as interprocessor communication and interlocks complete procedures for garbage collection and for standard list processing primitives are presented and thoroughly explained particular attention is given to the problems of marking and relocating list cells while another processor may be operating on them the primary aim throughout is to allow the list processor to run unimpeded while the other processor reclaims list storage the more complex cases involving several list processors and one or more garbage collection processors are also briefly discussed cacm september 1975 steele g l jr garbage collection storage reclamation reclaimer storage allocation multiprocessing synchronization semaphores parallel processing compactification relocation lisp list processing free storage pointers data structures gc processor 419 432 440 449 49 ca750901 jb january 6 1978 339 pm 
****************************
/Users/rohitchawla/PycharmProjects/tryingtodosomecrap/Corpus/CACM-2570
a comparison of list schedules for parallel processing systems the problem of scheduling two or more processors to minimize the execution time of a program which consists of a set of partially ordered tasks is studied cases where task execution times are deterministic and others in which execution times are random variables are analyzed it is shown that different algorithms suggested in the literature vary significantly in execution time and that the b schedule of coffman and graham is near optimal a dynamic programming solution for the case in which execution times are random variables is presented cacm december 1974 adam t l chandy k m dickson j r parallel processing precedence graphs scheduling list scheduling optimization dynamic programming 43 432 434 435 53 532 54 542 81 ca741204 jb january 16 1978 942 am 
****************************
/Users/rohitchawla/PycharmProjects/tryingtodosomecrap/Corpus/CACM-2973
sorting on a mesh connected parallel computer two algorithms are presented for sorting n2 elements on an n x n mesh connected processor array that require on routing and comparison steps the best previous algorithm takes time on log n the algorithms of this paper are shown to be optimal in time within small constant factors extensions to higher dimensional arrays are also given cacm april 1977 thompson c d kung h t parallel computer parallel sorting parallel merge routing and comparison steps perfect shuffle processor in terconnection pattern 432 525 531 ca770409 jb december 29 1977 458 am 
****************************
/Users/rohitchawla/PycharmProjects/tryingtodosomecrap/Corpus/CACM-1752
resource management for a medium scale time sharing operating system task scheduling and resource balancing for a medium size virtual memory paging machine are discussed in relation to a combined batch processing and time sharing environment a synopsis is given of the task scheduling and paging algorithms that were implemented and the results of comparative simulation are given by tracing the development of the algorithms through six predecessor versions throughout the discussion particular emphasis is placed on balancing the system performance relative to the characteristics of all the system resources simulation results relative to alternate hardware characteristics and the effects of program mix and loading variations are also presented cacm may 1968 oppenheimer g weizer n time sharing operating systems resource management task scheduling paging system simulation memory management virtual memories 430 431 432 ca680504 jb february 23 1978 939 am 
****************************
/Users/rohitchawla/PycharmProjects/tryingtodosomecrap/Corpus/CACM-2365
matrix computations with fortran and paging the efficiency of conventional fortran programs for matrix computations can often be improved by reversing the order of nested loops such modifications produce modest savings in many common situations and very significant savings for large problems run under an operating system which uses paging cacm april 1972 moler c b matrix algorithms linear equations fortran paged memory virtual memory array processing 422 432 514 ca720408 jb january 31 1978 1254 pm 
****************************
